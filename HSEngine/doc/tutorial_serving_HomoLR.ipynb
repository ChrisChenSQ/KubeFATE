{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "noticed-exchange",
   "metadata": {},
   "source": [
    "# HSEngine Tutorial\n",
    "HSEngine, or **H**orizontal **S**erving **Engine** or HSE, is a python package (`hsengine`) that provides a set of classes/APIs to help people to perform the end-to-end workflow of setting up a [KFServing](https://github.com/kubeflow/kfserving) service from a horizontally-trained model in [FATE](https://github.com/FederatedAI/FATE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-spoke",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* HSEngine by default expects that a KFServing installation is presented in the target kubernetes cluster.\n",
    "* Below example runs in a Jupyter Notebook pod in the target kubernetes cluster. Note HSEngine is not limited to this setup, as will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laden-ability",
   "metadata": {},
   "source": [
    "## High-level API: HSEngine\n",
    "The high-level class is called `HSEngine`. It encapsulate the complexity of all the steps. In the simplest, most typical case, a user only needs to provide the serving service name wanted, the service version string and the trained model version string recorded in FATE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "statewide-certificate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 06:53:45 fate_client:93] Downloading FATE model with role: guest, party_id: 3333, model_id: arbiter-3333#guest-3333#host-3333#model, model_version: 2021040701544681092419\n",
      "[I 210413 06:53:45 fate_client:113] FATE model is saved to: /tmp/tmpc3hw44qc\n",
      "[I 210413 06:53:46 fate_model:133] FATEModel initialized with component type: HomoLR and target framework: sklearn\n"
     ]
    }
   ],
   "source": [
    "# un-comment this to get more detailed logs\n",
    "# import logging\n",
    "# logger = logging.getLogger()\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "\n",
    "SERVICE_NAME = \"sklearn-lr\"\n",
    "VERSION = \"1.0\"\n",
    "\n",
    "from hsengine import HSEngine\n",
    "\n",
    "h = HSEngine(SERVICE_NAME,\n",
    "             VERSION, \n",
    "             model_version=\"2021040701544681092419\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-chicago",
   "metadata": {},
   "source": [
    "And then call the `run` method to launch everything. By default, a KFServing InferenceService will be setup in the default kubernetes cluster - the one that current Jupyter Notebook is running in. If you are not running this code in a kubernetes cluster, then you need to provide more parameters like the target cluster configuration file to HSEngine, refer to the later sections about low-level APIs and the complete HSEngine class definition for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dietary-insert",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 06:53:46 base:113] Preparing model storage and InferenceService spec...\n",
      "[I 210413 06:53:46 minio:158] Uploaded model objects into path: s3://models/sklearn-lr/1.0\n",
      "[I 210413 06:53:46 base:102] Prepared model with uri: s3://models/sklearn-lr/1.0\n",
      "[I 210413 06:53:46 base:145] InferenceService spec ready\n",
      "[I 210413 06:53:46 base:121] Creating InferenceService sklearn-lr...\n",
      "[I 210413 06:53:48 base:131] InferenceService: sklearn-lr created. To check service readiness, call this deployer's status(), wait() methods, or use KFServing query APIs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'serving.kubeflow.org/v1beta1',\n",
       " 'kind': 'InferenceService',\n",
       " 'metadata': {'annotations': {'hsengine.dev/uuid': 'bf63fab5-742e-4394-9027-3ff487b1df7e'},\n",
       "  'creationTimestamp': '2021-04-13T06:53:47Z',\n",
       "  'generation': 1,\n",
       "  'managedFields': [{'apiVersion': 'serving.kubeflow.org/v1beta1',\n",
       "    'fieldsType': 'FieldsV1',\n",
       "    'fieldsV1': {'f:metadata': {'f:annotations': {'.': {},\n",
       "       'f:hsengine.dev/uuid': {}}},\n",
       "     'f:spec': {'.': {},\n",
       "      'f:predictor': {'.': {},\n",
       "       'f:sklearn': {'.': {}, 'f:protocolVersion': {}, 'f:storageUri': {}}}}},\n",
       "    'manager': 'OpenAPI-Generator',\n",
       "    'operation': 'Update',\n",
       "    'time': '2021-04-13T06:53:46Z'}],\n",
       "  'name': 'sklearn-lr',\n",
       "  'namespace': 'fate-3333',\n",
       "  'resourceVersion': '5138823',\n",
       "  'uid': '8daf8ece-cbcd-4093-b4e7-e1f20eaab59b'},\n",
       " 'spec': {'predictor': {'sklearn': {'name': 'kfserving-container',\n",
       "    'protocolVersion': 'v1',\n",
       "    'resources': {'limits': {'cpu': '1', 'memory': '2Gi'},\n",
       "     'requests': {'cpu': '1', 'memory': '2Gi'}},\n",
       "    'runtimeVersion': 'v0.5.1',\n",
       "    'storageUri': 's3://models/sklearn-lr/1.0'}}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-monthly",
   "metadata": {},
   "source": [
    "We can then call the embedded deployer's (will be explained later) `wait` method to monitor the KFServing service status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "medium-vatican",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 READY      PREV                      LATEST                    URL                                                              \n",
      "sklearn-lr           Unknown                                                                                                                         \n",
      "sklearn-lr           Unknown                                                                                                                         \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           True       0                         100                       http://sklearn-lr.fate-3333.example.com                          \n"
     ]
    }
   ],
   "source": [
    "h.deployer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-appointment",
   "metadata": {},
   "source": [
    "The `HSEngine` contains a `trained_model`, a `converter` and a `deployer` object that work together for streamlining the workflow. Again, refer to the low-level APIs discussed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "activated-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'service_name': 'sklearn-lr',\n",
       " 'version': '1.0',\n",
       " 'converter_kwargs': {},\n",
       " 'deployer_kwargs': {},\n",
       " 'converter': <hsengine.converters.fate.fate_converter.FATEConverter at 0x7f8f9be8ffd0>,\n",
       " 'deployer': <hsengine.deployers.kfserving.sklearn.SKLearnV1KFDeployer at 0x7f8f9be8f290>,\n",
       " 'trained_model': <hsengine.trained_model.fate_model.FATEModel at 0x7f909c5a6f10>,\n",
       " 'converted_model': <hsengine.converters.converter_base.ConvertedModel at 0x7f8f9be8fb10>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scenic-lindsay",
   "metadata": {},
   "source": [
    "### Test the sevice\n",
    "\n",
    "Next couple cells is not related to HSEngine implementation. They compose a http request from testing data, and send the request to the serving service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "amended-optics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 30)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read data for prediction\n",
    "\n",
    "TEST_DATA_FILE = \"/data/projects/fate/examples/data/breast_homo_guest.csv\"\n",
    "import pandas\n",
    "data = pandas.read_csv(TEST_DATA_FILE).sort_values(by=['id'])\n",
    "X = data.values[:,2:]\n",
    "X.shape\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit(X).transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-mandate",
   "metadata": {},
   "source": [
    "Please change the values of `SERVICE_HOSTNAME`, `INGRESS_HOST`, `INGRESS_PORT` based on your cluster setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinguished-keeping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify the serving service\n",
    "import requests\n",
    "\n",
    "SERVICE_HOSTNAME = '{}.fate-3333.example.com'.format(SERVICE_NAME)\n",
    "INGRESS_HOST=\"10.182.130.98\"\n",
    "INGRESS_PORT=\"30273\"\n",
    "\n",
    "URL = f'http://{INGRESS_HOST}:{INGRESS_PORT}/v1/models/{SERVICE_NAME}:predict'\n",
    "    \n",
    "# Use the X\n",
    "INPUT={'instances':X.tolist()}\n",
    "res = requests.post(URL, json=INPUT, headers={'Host': SERVICE_HOSTNAME})\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-angel",
   "metadata": {},
   "source": [
    "The classification results are returned in the predict response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "grave-career",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  1.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  1.0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-appraisal",
   "metadata": {},
   "source": [
    "Now, if no longer needed, we can use the deployer's destroy method to shutdown the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "further-timber",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 06:54:23 base:155] InferenceService sklearn-lr is deleted\n"
     ]
    }
   ],
   "source": [
    "h.deployer.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-bible",
   "metadata": {},
   "source": [
    "An end-to-end workflow of `HSEngine` is relatively straightforward. As the log lines in above example indicate, the \"engine\" fetches the FATE model files from FATE server (the fate flow server), then converts the model to a model object of a common framework, and finally uses kubernetes and KFServing APIs to setup a serving service in a cluster.\n",
    "\n",
    "Next let's dig into the classes that perform that actual work above, known as the **low-level APIs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-python",
   "metadata": {},
   "source": [
    "## Low-level APIs\n",
    "\n",
    "Within the HSEngine, there are three main classes: `FATEModel`, `FATEConverter` and `KFServingDeployer`. If more complicated deployment senarios or configurations are needed, these classes can be used directly, instead of using the high-level API.\n",
    "\n",
    "All the configurations are actually exposed via the high-level API too, please refer to the docs of the HSEngine class for how to work with it. (or see the appendix at the end of this tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-mumbai",
   "metadata": {},
   "source": [
    "### Low-level APIs: FATEModel\n",
    "\n",
    "Below is a FATEModel example, people can specify the local model path, a in-memory model dict or simply provide the model version and the FATE flow server access info. For the last case, the model will be downloaded from FATE flow server.\n",
    "\n",
    "Finally, a model dict instance will be created and saved inside the `FATEModel` object. Any future workflow will be based on this \"model dict\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "mechanical-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 06:54:39 fate_client:93] Downloading FATE model with role: guest, party_id: 3333, model_id: arbiter-3333#guest-3333#host-3333#model, model_version: 2021040701544681092419\n",
      "[I 210413 06:54:40 fate_client:113] FATE model is saved to: /tmp/tmpw9c3jaax\n",
      "[I 210413 06:54:40 fate_model:133] FATEModel initialized with component type: HomoLR and target framework: sklearn\n"
     ]
    }
   ],
   "source": [
    "from hsengine.trained_model.fate_model import FATEModel\n",
    "\n",
    "fm = FATEModel(model_path=None,\n",
    "               model_dict=None,\n",
    "               model_version=\"2021040701544681092419\",\n",
    "               fate_flow_host=None,\n",
    "               fate_flow_port=None,\n",
    "               api_version=\"v1\",\n",
    "               role=\"guest\",\n",
    "               party_id=None,\n",
    "               model_id=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-substitute",
   "metadata": {},
   "source": [
    "### Low-level APIs: FATEConverter\n",
    "\n",
    "A `FATEConverter` accept a `FATEModel` as input. And its `convert` method will give a model object of the common framework, based on the type of the original `FATEModel`.\n",
    "\n",
    "Currenty, `HomoLR` model will be converted to scikit-learn's `LogisticRegression` model. And for \"HomoNN\", a `tensorflow.keras` model or `torch.nn` model will be returned based on the original trained model within FATE.\n",
    "\n",
    "`model_storage` related topics will be covered later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "earlier-jacob",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsengine.converters.fate.fate_converter import FATEConverter\n",
    "from hsengine.integration.model_storage import ModelStorageType\n",
    "\n",
    "fc = FATEConverter(fm,\n",
    "                   model_storage_type=ModelStorageType.LOCAL_FILE,\n",
    "                   model_storage_kwargs=None)\n",
    "\n",
    "converted_model = fc.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decimal-truth",
   "metadata": {},
   "source": [
    "`converted_model` is of type `ConvertedModel` that contains two attribute:  \n",
    "* `framework`: a string indicating the ML framework of the model.  \n",
    "* `model`: the converted model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afraid-presence",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'framework': 'sklearn',\n",
       " 'model': LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "           intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "           penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "           verbose=0, warm_start=False)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(converted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-recording",
   "metadata": {},
   "source": [
    "`FATEModel` also has a `save_model` method, that by default will serialize the converted model - for example, using joblib to dump the sklearn model - to save it into local filesystem for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "least-timer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 06:54:51 base:41] Saved model of framework: sklearn into /tmp/saved_model.joblib\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/tmp/saved_model.joblib'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc.save_model(\"/tmp/saved_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-interpretation",
   "metadata": {},
   "source": [
    "Sometimes, we can stop here. `FATEConverter` is all we need - we only needs the converted model object and we can then do whatever we want with it - save it to local path, do local prediction directly, etc.\n",
    "\n",
    "If we want to use KFServing to serve our model, then go on to the next section about deployers classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-background",
   "metadata": {},
   "source": [
    "### Low-level APIs: KFServingDeployer & sub-classes\n",
    "\n",
    "A `KFServingDeployer` is in charge of taking the converted model and setting up a serving service from it. It is an abstract class, that people should not use it directly. A convenient method `get_kfserving_deployer` can be used to get the sub-class deployer instance according to the framework of the converted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enormous-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hsengine.deployers.kfserving.sklearn.SKLearnV1KFDeployer"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hsengine.deployers.kfserving import get_kfserving_deployer\n",
    "\n",
    "deployer = get_kfserving_deployer(service_name=SERVICE_NAME, \n",
    "                                  version=VERSION, \n",
    "                                  converted_model=converted_model,\n",
    "                                  protocol_version=\"v1\",\n",
    "                                  model_storage_type=ModelStorageType.MINIO,\n",
    "                                  model_storage_kwargs=None,\n",
    "                                  storage_uri=None,\n",
    "                                  isvc=None,\n",
    "                                  kfserving_config=None,\n",
    "                                  replace=True,\n",
    "                                  framework_kwargs=None) # for specific server config\n",
    "type(deployer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-authentication",
   "metadata": {},
   "source": [
    "The `KFServingDeployer` contains several methods to do the setup. The most common ones are `deploy`, `status`, `wait` and `destroy` methods, performing the tasks as their names indicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "complete-child",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 07:01:52 base:113] Preparing model storage and InferenceService spec...\n",
      "[I 210413 07:01:52 minio:158] Uploaded model objects into path: s3://models/sklearn-lr/1.0\n",
      "[I 210413 07:01:52 base:102] Prepared model with uri: s3://models/sklearn-lr/1.0\n",
      "[I 210413 07:01:52 base:145] InferenceService spec ready\n",
      "[I 210413 07:01:52 base:121] Creating InferenceService sklearn-lr...\n",
      "[I 210413 07:01:54 base:131] InferenceService: sklearn-lr created. To check service readiness, call this deployer's status(), wait() methods, or use KFServing query APIs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                 READY      PREV                      LATEST                    URL                                                              \n",
      "sklearn-lr           Unknown                                                                                                                         \n",
      "sklearn-lr           Unknown                                                                                                                         \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           Unknown    0                         100                                                                                        \n",
      "sklearn-lr           True       0                         100                       http://sklearn-lr.fate-3333.example.com                          \n"
     ]
    }
   ],
   "source": [
    "deployer.deploy()\n",
    "deployer.wait()\n",
    "# deployer.status()\n",
    "# deployer.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worth-stopping",
   "metadata": {},
   "source": [
    "Beside these methods, people can further customize the service by editing the InferenceService (isvc) object embedded ini the deployer directly.\n",
    "\n",
    "* First, call the `prepare_model` method to prepare the model files needed for the service. This step is required by KFServing - it needs a url to fetch the model file. Different framework requires different model files layout, and our deployer classes here take care of it.\n",
    "  * Sometimes this is enough - it will returns a uri that can be used to set up an InferenceService. People who are familiar with KFServing can use this uri to manually create the service in different kubernetes clusters.\n",
    "* Then, call the `prepare_isvc` method to generate the isvc object based on the framework and the model storage uri we just prepared.\n",
    "* Now we can get the isvc object and add whatever changes to it as we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "variable-management",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 07:02:21 base:155] InferenceService sklearn-lr is deleted\n",
      "[I 210413 07:02:21 minio:158] Uploaded model objects into path: s3://models/sklearn-lr/1.0\n",
      "[I 210413 07:02:21 base:102] Prepared model with uri: s3://models/sklearn-lr/1.0\n",
      "[I 210413 07:02:21 base:145] InferenceService spec ready\n"
     ]
    }
   ],
   "source": [
    "deployer.destroy()\n",
    "deployer = get_kfserving_deployer(service_name=SERVICE_NAME, \n",
    "                                  version=VERSION, \n",
    "                                  converted_model=converted_model,\n",
    "                                  protocol_version=\"v1\",\n",
    "                                  model_storage_type=ModelStorageType.MINIO,\n",
    "                                  model_storage_kwargs=None,\n",
    "                                  storage_uri=None,\n",
    "                                  isvc=None,\n",
    "                                  kfserving_config=None,\n",
    "                                  replace=True,\n",
    "                                  framework_kwargs=None) # for specific server config\n",
    "\n",
    "deployer.prepare_model()\n",
    "deployer.prepare_isvc()\n",
    "isvc = deployer.isvc\n",
    "# isvc.some_attribute = some_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "given-interface",
   "metadata": {},
   "source": [
    "* Finally we call `deploy` method to deploy our customized isvc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "brazilian-latino",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 210413 07:16:47 base:121] Creating InferenceService sklearn-lr...\n",
      "[I 210413 07:16:49 base:131] InferenceService: sklearn-lr created. To check service readiness, call this deployer's status(), wait() methods, or use KFServing query APIs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'serving.kubeflow.org/v1beta1',\n",
       " 'kind': 'InferenceService',\n",
       " 'metadata': {'annotations': {'hsengine.dev/uuid': '11152016-e850-4ad3-b498-68f4b2282e43'},\n",
       "  'creationTimestamp': '2021-04-13T07:16:49Z',\n",
       "  'generation': 1,\n",
       "  'managedFields': [{'apiVersion': 'serving.kubeflow.org/v1beta1',\n",
       "    'fieldsType': 'FieldsV1',\n",
       "    'fieldsV1': {'f:metadata': {'f:annotations': {'.': {},\n",
       "       'f:hsengine.dev/uuid': {}}},\n",
       "     'f:spec': {'.': {},\n",
       "      'f:predictor': {'.': {},\n",
       "       'f:sklearn': {'.': {}, 'f:protocolVersion': {}, 'f:storageUri': {}}}}},\n",
       "    'manager': 'OpenAPI-Generator',\n",
       "    'operation': 'Update',\n",
       "    'time': '2021-04-13T07:16:47Z'}],\n",
       "  'name': 'sklearn-lr',\n",
       "  'namespace': 'fate-3333',\n",
       "  'resourceVersion': '5154815',\n",
       "  'uid': '9433c0b8-475d-488b-9714-0a8e2c5ce23a'},\n",
       " 'spec': {'predictor': {'sklearn': {'name': 'kfserving-container',\n",
       "    'protocolVersion': 'v1',\n",
       "    'resources': {'limits': {'cpu': '1', 'memory': '2Gi'},\n",
       "     'requests': {'cpu': '1', 'memory': '2Gi'}},\n",
       "    'runtimeVersion': 'v0.5.1',\n",
       "    'storageUri': 's3://models/sklearn-lr/1.0'}}}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deployer.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-insider",
   "metadata": {},
   "source": [
    "### Low-level APIs: BaseModelStorage & MinIOModelStorage\n",
    "\n",
    "Model storage represents an interface to save the model files. By default the `BaseModelStorage` is used by the `FATEConverter` to save model to local path. And `MinIOModelStorage` is used by `KFServingDeployer` to upload model files for future serving use.\n",
    "\n",
    "Refer to the doc of each class for detailed explanation of the parameters. For example, the `KFServingDeployer` uses code similar to the following cell to initialize its inner model storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "prescription-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hsengine.integration.model_storage import *\n",
    "minio_storage = MinIOModelStorage(sub_path=\"\",\n",
    "                                  framework=\"\",\n",
    "                                  bucket=\"models\",\n",
    "                                  endpoint=None,\n",
    "                                  access_key=None,\n",
    "                                  secret_key=None,\n",
    "                                  region=None,\n",
    "                                  secure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-consciousness",
   "metadata": {},
   "source": [
    "# Appendix A: HSEngine signature\n",
    "A illustrated above, the low-level APIs expose lots of paramters. When using the HSEngine class, all these parameter can be provided too, though not very straightforward. Here is what the HSEngine initilization function looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "maritime-beverage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class HSEngine in module hsengine.hsengine:\n",
      "\n",
      "class HSEngine(builtins.object)\n",
      " |  HSEngine(service_name, version='1.0', converter_kwargs=None, deployer_kwargs=None, **kwargs)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, service_name, version='1.0', converter_kwargs=None, deployer_kwargs=None, **kwargs)\n",
      " |      :param service_name: the service name\n",
      " |      :param version: the service version, default to \"1.0\"\n",
      " |      :param converter_kwargs: a dict of keyword arguments that will be used to infer\n",
      " |                               and initialize the converter. See the doc of get_converter\n",
      " |                               and subclasses of ConverterBase for supported arguments.\n",
      " |      :param deployer_kwargs: a dict of keyword arguments that will be used to infer\n",
      " |                              the deployer type and initialize it. See the doc of\n",
      " |                              get_deployer() and subclasses of the ConverterBase class\n",
      " |                              for supported arguments.\n",
      " |      :param kwargs: keyword arguments to describe the original trained model. See the\n",
      " |                     doc of the derived TrainedModel class for supported arguments.\n",
      " |  \n",
      " |  infer_converter(self)\n",
      " |  \n",
      " |  infer_deployer(self)\n",
      " |  \n",
      " |  run(self)\n",
      " |      Starts the engine\n",
      " |      \n",
      " |      May auto-create converter and deployer\n",
      " |      :returns: the deployed service\n",
      " |      :rtype: depends on the deployer instance\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(HSEngine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-tribe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
